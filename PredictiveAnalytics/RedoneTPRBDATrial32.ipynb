{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Data Analytics Project\n",
    "\n",
    "18MCMI05: Delton M Antony, MTech Artificial Intelligence\n",
    "\n",
    "18MCMI14: Garima Jain, MTech Artificial Intelligence\n",
    "\n",
    "Churn Prediction Dataset\n",
    "\n",
    "Each row represents a customer, each column contains customer’s attributes described on the column Metadata.\n",
    "The raw data contains 7043 rows (customers) and 21 columns (features).\n",
    "The “Churn” column is our target\n",
    "\n",
    "Classification:\n",
    "\n",
    "Classification is a supervised learning approach in which the computer program learns from the data input given to it and then uses this learning to classify new observation. Here, in the telecom churn prediction dataset, the problem is to predict whether the customer is going to churn or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# read into a pandas dataframe\n",
    "data = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first few records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ...  DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...                No   \n",
       "1                No             DSL            Yes  ...               Yes   \n",
       "2                No             DSL            Yes  ...                No   \n",
       "3  No phone service             DSL            Yes  ...               Yes   \n",
       "4                No     Fiber optic             No  ...                No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for the datatype of the features and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      "customerID          7043 non-null object\n",
      "gender              7043 non-null object\n",
      "SeniorCitizen       7043 non-null int64\n",
      "Partner             7043 non-null object\n",
      "Dependents          7043 non-null object\n",
      "tenure              7043 non-null int64\n",
      "PhoneService        7043 non-null object\n",
      "MultipleLines       7043 non-null object\n",
      "InternetService     7043 non-null object\n",
      "OnlineSecurity      7043 non-null object\n",
      "OnlineBackup        7043 non-null object\n",
      "DeviceProtection    7043 non-null object\n",
      "TechSupport         7043 non-null object\n",
      "StreamingTV         7043 non-null object\n",
      "StreamingMovies     7043 non-null object\n",
      "Contract            7043 non-null object\n",
      "PaperlessBilling    7043 non-null object\n",
      "PaymentMethod       7043 non-null object\n",
      "MonthlyCharges      7043 non-null float64\n",
      "TotalCharges        7043 non-null object\n",
      "Churn               7043 non-null object\n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing:\n",
    "We need to correct the datatype of features, check and impute for null values, perform data partitioning, handle categorical data, perform feature selection and feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the total charges in a non-null object. We need to convert it into a numerical datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TotalCharges    11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulls = data.isnull().sum()\n",
    "nulls[nulls > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are eleven null values in 'TotalCharges'. Let us impute these null values with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'no phone service' value in the above MultipleLines independent variable can be treated as 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MultipleLines'].replace('No phone service','No',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let y be the vector of dependent variable values of \"Churn\" and Let X be the matrix heading all the independent variables. Split the data into train and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Churn'].map({'Yes':1,'No':0})\n",
    "X = data.drop(labels=['Churn','customerID'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenure, MonthlyCharges and TotalCharges are not categorical. Every other column is categorical. Hence, we need to convert the categorical columns into binary. First find the list of categorical columns for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'SeniorCitizen',\n",
       " 'Partner',\n",
       " 'Dependents',\n",
       " 'PhoneService',\n",
       " 'MultipleLines',\n",
       " 'InternetService',\n",
       " 'OnlineSecurity',\n",
       " 'OnlineBackup',\n",
       " 'DeviceProtection',\n",
       " 'TechSupport',\n",
       " 'StreamingTV',\n",
       " 'StreamingMovies',\n",
       " 'Contract',\n",
       " 'PaperlessBilling',\n",
       " 'PaymentMethod']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = []\n",
    "for column in X.columns:\n",
    "    if column not in ['tenure','MonthlyCharges','TotalCharges']:\n",
    "        cat_cols.append(column)\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above columns are categorical columns. Now, you can convert these into binary by either of the two means - OneHotEncoder or pandas.get_dummies(). Encoding the categorical columns with the pandas get_dummies() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 45 columns):\n",
      "tenure                                     7043 non-null int64\n",
      "MonthlyCharges                             7043 non-null float64\n",
      "TotalCharges                               7043 non-null float64\n",
      "gender_Female                              7043 non-null uint8\n",
      "gender_Male                                7043 non-null uint8\n",
      "SeniorCitizen_0                            7043 non-null uint8\n",
      "SeniorCitizen_1                            7043 non-null uint8\n",
      "Partner_No                                 7043 non-null uint8\n",
      "Partner_Yes                                7043 non-null uint8\n",
      "Dependents_No                              7043 non-null uint8\n",
      "Dependents_Yes                             7043 non-null uint8\n",
      "PhoneService_No                            7043 non-null uint8\n",
      "PhoneService_Yes                           7043 non-null uint8\n",
      "MultipleLines_No                           7043 non-null uint8\n",
      "MultipleLines_Yes                          7043 non-null uint8\n",
      "InternetService_DSL                        7043 non-null uint8\n",
      "InternetService_Fiber optic                7043 non-null uint8\n",
      "InternetService_No                         7043 non-null uint8\n",
      "OnlineSecurity_No                          7043 non-null uint8\n",
      "OnlineSecurity_No internet service         7043 non-null uint8\n",
      "OnlineSecurity_Yes                         7043 non-null uint8\n",
      "OnlineBackup_No                            7043 non-null uint8\n",
      "OnlineBackup_No internet service           7043 non-null uint8\n",
      "OnlineBackup_Yes                           7043 non-null uint8\n",
      "DeviceProtection_No                        7043 non-null uint8\n",
      "DeviceProtection_No internet service       7043 non-null uint8\n",
      "DeviceProtection_Yes                       7043 non-null uint8\n",
      "TechSupport_No                             7043 non-null uint8\n",
      "TechSupport_No internet service            7043 non-null uint8\n",
      "TechSupport_Yes                            7043 non-null uint8\n",
      "StreamingTV_No                             7043 non-null uint8\n",
      "StreamingTV_No internet service            7043 non-null uint8\n",
      "StreamingTV_Yes                            7043 non-null uint8\n",
      "StreamingMovies_No                         7043 non-null uint8\n",
      "StreamingMovies_No internet service        7043 non-null uint8\n",
      "StreamingMovies_Yes                        7043 non-null uint8\n",
      "Contract_Month-to-month                    7043 non-null uint8\n",
      "Contract_One year                          7043 non-null uint8\n",
      "Contract_Two year                          7043 non-null uint8\n",
      "PaperlessBilling_No                        7043 non-null uint8\n",
      "PaperlessBilling_Yes                       7043 non-null uint8\n",
      "PaymentMethod_Bank transfer (automatic)    7043 non-null uint8\n",
      "PaymentMethod_Credit card (automatic)      7043 non-null uint8\n",
      "PaymentMethod_Electronic check             7043 non-null uint8\n",
      "PaymentMethod_Mailed check                 7043 non-null uint8\n",
      "dtypes: float64(2), int64(1), uint8(42)\n",
      "memory usage: 454.0 KB\n"
     ]
    }
   ],
   "source": [
    "X= pd.get_dummies(X,columns=cat_cols)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the columns ie independent variables we got after handling categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to perform feature selection in the data. Out of the many multiple ways to perform feature selection, I am using Backward Elimination using OLS ie Ordinary Least Squares. I am performing it manually so that I can see the output of the p value after removing each feature. This is done by using the summary() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append a column of ones as the 0th column so that it can work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(arr= np.ones((7043,1)).astype(int), values = X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the preparation to perform backward elimination is complete. The procedure is to fit using OLS and check the summary to see the p value. During backward elimination, all the features are taken all at once at first. Then features are dropped one by one by dropping the feature with the highest p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Churn</td>      <th>  R-squared:         </th> <td>   0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   163.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 05 Nov 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:39:20</td>     <th>  Log-Likelihood:    </th> <td> -3064.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  7043</td>      <th>  AIC:               </th> <td>   6165.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  7025</td>      <th>  BIC:               </th> <td>   6289.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    17</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.0597</td> <td>    0.006</td> <td>   10.597</td> <td> 0.000</td> <td>    0.049</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.0014</td> <td>    0.000</td> <td>   -3.032</td> <td> 0.002</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>-4.831e-05</td> <td> 6.23e-06</td> <td>   -7.748</td> <td> 0.000</td> <td>-6.05e-05</td> <td>-3.61e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0317</td> <td>    0.005</td> <td>    6.040</td> <td> 0.000</td> <td>    0.021</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0280</td> <td>    0.005</td> <td>    5.279</td> <td> 0.000</td> <td>    0.018</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0487</td> <td>    0.013</td> <td>    3.822</td> <td> 0.000</td> <td>    0.024</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0345</td> <td>    0.005</td> <td>    6.466</td> <td> 0.000</td> <td>    0.024</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.0252</td> <td>    0.006</td> <td>    4.313</td> <td> 0.000</td> <td>    0.014</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0490</td> <td>    0.011</td> <td>    4.611</td> <td> 0.000</td> <td>    0.028</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -0.0540</td> <td>    0.006</td> <td>   -8.959</td> <td> 0.000</td> <td>   -0.066</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.1190</td> <td>    0.009</td> <td>   13.855</td> <td> 0.000</td> <td>    0.102</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0487</td> <td>    0.012</td> <td>    4.065</td> <td> 0.000</td> <td>    0.025</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.0415</td> <td>    0.006</td> <td>    6.880</td> <td> 0.000</td> <td>    0.030</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0235</td> <td>    0.007</td> <td>    3.385</td> <td> 0.001</td> <td>    0.010</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.0324</td> <td>    0.006</td> <td>    5.213</td> <td> 0.000</td> <td>    0.020</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0325</td> <td>    0.007</td> <td>    4.656</td> <td> 0.000</td> <td>    0.019</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0476</td> <td>    0.012</td> <td>    3.928</td> <td> 0.000</td> <td>    0.024</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.0523</td> <td>    0.012</td> <td>    4.358</td> <td> 0.000</td> <td>    0.029</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>   -0.0052</td> <td>    0.002</td> <td>   -2.552</td> <td> 0.011</td> <td>   -0.009</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0561</td> <td>    0.012</td> <td>    4.673</td> <td> 0.000</td> <td>    0.033</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>    0.1000</td> <td>    0.013</td> <td>    7.485</td> <td> 0.000</td> <td>    0.074</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0448</td> <td>    0.010</td> <td>    4.495</td> <td> 0.000</td> <td>    0.025</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>    0.0716</td> <td>    0.011</td> <td>    6.641</td> <td> 0.000</td> <td>    0.050</td> <td>    0.093</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>359.700</td> <th>  Durbin-Watson:     </th> <td>   2.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 384.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.545</td>  <th>  Prob(JB):          </th> <td>3.09e-84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.653</td>  <th>  Cond. No.          </th> <td>3.26e+19</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 6.84e-29. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Churn   R-squared:                       0.283\n",
       "Model:                            OLS   Adj. R-squared:                  0.281\n",
       "Method:                 Least Squares   F-statistic:                     163.0\n",
       "Date:                Mon, 05 Nov 2018   Prob (F-statistic):               0.00\n",
       "Time:                        01:39:20   Log-Likelihood:                -3064.7\n",
       "No. Observations:                7043   AIC:                             6165.\n",
       "Df Residuals:                    7025   BIC:                             6289.\n",
       "Df Model:                          17                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0597      0.006     10.597      0.000       0.049       0.071\n",
       "x1            -0.0014      0.000     -3.032      0.002      -0.002      -0.000\n",
       "x2         -4.831e-05   6.23e-06     -7.748      0.000   -6.05e-05   -3.61e-05\n",
       "x3             0.0317      0.005      6.040      0.000       0.021       0.042\n",
       "x4             0.0280      0.005      5.279      0.000       0.018       0.038\n",
       "x5             0.0487      0.013      3.822      0.000       0.024       0.074\n",
       "x6             0.0345      0.005      6.466      0.000       0.024       0.045\n",
       "x7             0.0252      0.006      4.313      0.000       0.014       0.037\n",
       "x8             0.0490      0.011      4.611      0.000       0.028       0.070\n",
       "x9            -0.0540      0.006     -8.959      0.000      -0.066      -0.042\n",
       "x10            0.1190      0.009     13.855      0.000       0.102       0.136\n",
       "x11           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x12            0.0487      0.012      4.065      0.000       0.025       0.072\n",
       "x13           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x14            0.0415      0.006      6.880      0.000       0.030       0.053\n",
       "x15           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x16            0.0235      0.007      3.385      0.001       0.010       0.037\n",
       "x17            0.0324      0.006      5.213      0.000       0.020       0.045\n",
       "x18           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x19            0.0325      0.007      4.656      0.000       0.019       0.046\n",
       "x20            0.0476      0.012      3.928      0.000       0.024       0.071\n",
       "x21           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x22           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x23            0.0523      0.012      4.358      0.000       0.029       0.076\n",
       "x24           -0.0052      0.002     -2.552      0.011      -0.009      -0.001\n",
       "x25            0.0561      0.012      4.673      0.000       0.033       0.080\n",
       "x26            0.1000      0.013      7.485      0.000       0.074       0.126\n",
       "x27            0.0448      0.010      4.495      0.000       0.025       0.064\n",
       "x28            0.0716      0.011      6.641      0.000       0.050       0.093\n",
       "==============================================================================\n",
       "Omnibus:                      359.700   Durbin-Watson:                   2.005\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              384.576\n",
       "Skew:                           0.545   Prob(JB):                     3.09e-84\n",
       "Kurtosis:                       2.653   Cond. No.                     3.26e+19\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 6.84e-29. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt = X\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p value is for col 42\n",
    "#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p value is for col 14, so delete that\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p value is for col 20, so delete that\n",
    "X_opt = X_opt[:,[0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p s for col 32, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 28, delete it\n",
    "#X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 35, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 37, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 38, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 28, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 19, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 5, delete it\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 32, delete it. Highest p was .452\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 11, delete it. Highest p was .178\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 10, delete it. Highest p was .083\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 29, delete it. Highest p was .019\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 9, delete it. Highest p was .013\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 8, delete it. Highest p was .075\n",
    "#X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "X_opt = X_opt[:,[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()  # OrdinaryLeastSquares\n",
    "regressor_OLS.summary() # Highest p is for col 8, delete it. Highest p was .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered out the features with large p values until the maximum p value became 0.01. Now we are left with 28 features. The only thing left here is to overwrite our input matrix with this optimal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling:\n",
    "It is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm. Here we are standardizing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.27744458, -0.99261052, ...,  0.90418382,\n",
       "         0.8297975 ,  1.40641839],\n",
       "       [ 0.        ,  0.06632742, -0.17216471, ..., -1.1059698 ,\n",
       "        -1.20511329, -0.71102597],\n",
       "       [ 0.        , -1.23672422, -0.9580659 , ...,  0.90418382,\n",
       "         0.8297975 , -0.71102597],\n",
       "       ...,\n",
       "       [ 0.        , -0.87024095, -0.85293201, ...,  0.90418382,\n",
       "         0.8297975 ,  1.40641839],\n",
       "       [ 0.        , -1.15528349, -0.87051315, ...,  0.90418382,\n",
       "         0.8297975 , -0.71102597],\n",
       "       [ 0.        ,  1.36937906,  2.01389665, ..., -1.1059698 ,\n",
       "         0.8297975 , -0.71102597]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the data got scaled from the above representation of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data partitioning:\n",
    "Now, split the data into training set and test set. It is split in the ratio 7:3. The resultant data is stored into training data (X_train, y_train) and test data (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, the Data Preprocessing stage is now complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling:\n",
    "\n",
    "\n",
    "This is a classification problem. It means that we need to classify each output into two classes in this case, which is called binary classification. For binary classification, we can apply Logistic regression, Naive Bayes, Support Vector Machine, Decision Tree, Random Forest, Multi Layer Perceptron and Light and Extreme Gradient Boosting methods to make respective models that can classify the records into Churn or Not Churn.\n",
    "The hyperparameters applied to the models are selected using grid search done off record.\n",
    "In some models we can note that it gives best results with default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "With logistic regression, we are trying to fit the data to the logistic function. It is a discriminative classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import stdev, pstdev, mean, pvariance\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the vector y_predLogReg contains the predicted results. Evaluating the classification results can be done in two ways - by using a confusion matrix or by using an roc curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cross_val_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-386292540922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cross_val_score'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performCrossValidation(classifier,X,y):\n",
    "    numberOfFolds=10\n",
    "    kfold = KFold(numberOfFolds,False, 1)\n",
    "    tprarr=[]\n",
    "    for train, test in kfold.split(X):\n",
    "        trainingSet=X[train]\n",
    "        testingSet=X[test]   \n",
    "        lablesTrain=y[train]\n",
    "        lablesTest=y[test]\n",
    "        classifier.fit(trainingSet, lablesTrain)\n",
    "        predicted=classifier.predict(testingSet)\n",
    "        fpr, tpr, thresholds = roc_curve(predicted, lablesTest)\n",
    "        tprarr.append(tpr[1])\n",
    "    return tprarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logRegClassifier = LogisticRegression(random_state=0)\n",
    "logRegClassifier_unfit = LogisticRegression(random_state=0)\n",
    "# print(logRegClassifier)\n",
    "tpr_val={}\n",
    "\n",
    "tprLogReg=performCrossValidation(logRegClassifier,X,y)\n",
    "# tpr_val.append(tprLogReg)\n",
    "tpr_val.update({'tprLogReg':tprLogReg})\n",
    "# print(tpr_val)\n",
    "logRegClassifier.fit(X, y)\n",
    "# print(logRegClassifier)\n",
    "# y_predLogReg = logRegClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predLogReg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2fb7ba61494d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogRegConfMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predLogReg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlogRegConfMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_predLogReg' is not defined"
     ]
    }
   ],
   "source": [
    "logRegConfMatrix = confusion_matrix(y_test, y_predLogReg)\n",
    "logRegConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, accuracy = 80.075 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, accuracy is not good enough to calculate the goodness of model. What if the model simply predicts every customer as no churning - ie all predicted values are 0s. Then still the accuracy score will be high. Hence we need measures to make sure that the evaluation of a model is foolproof. That is where the auc comes into play. Area Under roc Curve gives a better perspective into the dependability of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predLogReg, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC for logistic regression model is 0.74 and the accuracy is 80.075%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predLogReg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: What proportion of positive identifications was actually correct?\n",
    "Precision is defined as TP/(TP+FP)\n",
    "\n",
    "Recall: What proportion of actual positives was identified correctly?\n",
    "Recall is defined as TP/(TP+FN)\n",
    "Recall is also called Sensitivity\n",
    "\n",
    "f1-score: The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.\n",
    "f1-score = 2*(Recall*Precision)/(Recall+Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can't say conclusively that the above accuracy is the actual accuracy. We are only considering a portion of the data as the test set. To get the mean accuracy, we have to apply cross validation. With k fold cross validation, we divide the data into k chunks and then one of these chunks as test set while keeping the rest as training. We iteratively consider each of these chunks as test set and will calculate the accuracies and its mean. This mean will give you the actual accuracy over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8062906880454601\n",
      "0.8068986976006853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "logRegaccuracies = cross_val_score(estimator=logRegClassifier, X=X_train, y=y_train, cv=10)\n",
    "logRegaccuracies_unfit = cross_val_score(estimator=logRegClassifier_unfit, X=X, y=y, cv=10)\n",
    "print(logRegaccuracies.mean())\n",
    "print(logRegaccuracies_unfit.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}#store 10FCV accuracy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update({'logReg':logRegaccuracies_unfit.mean()})\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the individual accuracies for each of the folds. The actual accuracy of the classifier across the entire dataset is given by the mean which is calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegAccuracy = logRegaccuracies.mean()\n",
    "logRegAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classification:\n",
    "Naive Bayes is a generative probability classifier. It works using the principle of Bayes Theorem for conditional probability under the assumption that the occurence of one feature has got nothing to do with the occurence of another in a record. This assumption is the reason why it is called Naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbClassifier = GaussianNB(priors=None)\n",
    "\n",
    "nbClassifier_unfit = GaussianNB(priors=None)\n",
    "\n",
    "tprnb=performCrossValidation(nbClassifier,X,y)\n",
    "# tpr_val.append(tprnb)\n",
    "tpr_val.update({'tprnb':tprnb})\n",
    "print(tpr_val)\n",
    "# print(len(tpr_val))\n",
    "nbClassifier.fit(X_train, y_train)\n",
    "y_predNB = nbClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the y_predNB vector contains the values predicted by naive bayes classifier. Let's evaluate the model using confusion matrix and roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbConfMatrix = confusion_matrix(y_test, y_predNB)\n",
    "nbConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, accuracy is 67.43 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc curve and finding the area under roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predNB, y_test)\n",
    "print(tpr[1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Naive Bayes (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us find the mean accuracy by cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbaccuracies = cross_val_score(estimator=nbClassifier, X=X_train, y=y_train, cv=10)\n",
    "nbaccuracies_unfit = cross_val_score(estimator=nbClassifier_unfit, X=X, y=y, cv=10)\n",
    "# nbaccuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update({'NB':nbaccuracies_unfit.mean()})\n",
    "d\n",
    "d.pop('nbaccuracies_unfit',None)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy after cross validation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classification:\n",
    "KNN stands for k-nearest neighbor.\n",
    "KNN is a lazy, non parametric algorithm.\n",
    "Let us consider 5 neighbors and use euclidian distance (ie minkowski distance with p = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNNClassifier = KNeighborsClassifier(n_neighbors=20, p=2, metric='minkowski')\n",
    "KNNClassifier_unfit = KNeighborsClassifier(n_neighbors=20, p=2, metric='minkowski')\n",
    "\n",
    "tprKNN=performCrossValidation(KNNClassifier,X,y)\n",
    "# print(tprKNN)\n",
    "tpr_val.update({'tprKNN':tprKNN})\n",
    "# print(len(tpr_val))\n",
    "tpr_val\n",
    "# KNNClassifier.fit(X_train, y_train)\n",
    "# y_predKNN = KNNClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_predKNN contains the predicted results. Let's evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnConfMatrix = confusion_matrix(y_test, y_predKNN)\n",
    "knnConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, accuracy = 78.845%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the roc and find the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predKNN, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='KNN (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predKNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the mean accuracy by doing 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knnaccuracies = cross_val_score(estimator=KNNClassifier, X=X_train, y=y_train, cv=10)\n",
    "knnaccuracies_unfit = cross_val_score(estimator=KNNClassifier_unfit, X=X, y=y, cv=10)\n",
    "# knnaccuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN10=cross_val_score(estimator=KNNClassifier, X=X, y=y, cv=10)\n",
    "d.update({'KNN':knnaccuracies_unfit.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine Classification:\n",
    "Unlike other models, SVC does not use the entire dataset to train itself. it uses the tipping points - also know an support vectors. These are those points that define the boundary of the two classes. This is a rare case of dimension reduction with respect to the records as opposed to features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svcClassifier = SVC(kernel='linear', random_state=0, cache_size=7000)\n",
    "svcClassifier_unfit = SVC(kernel='linear', random_state=0, cache_size=7000)\n",
    "\n",
    "tprSVC=performCrossValidation(svcClassifier,X,y)\n",
    "# print(tprSVC)\n",
    "tpr_val.update({'tprSVC':tprSVC})\n",
    "# print(len(tpr_val))\n",
    "print(tpr_val)\n",
    "# svcClassifier.fit(X_train, y_train)\n",
    "# y_predSVM = svcClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions made by SVM is stored in y_predSVM, it can be evaluated against the test set.\n",
    "The confusion matrix is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcConfMatrix = confusion_matrix(y_test, y_predSVM)\n",
    "svcConfMatrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here the accuracy is 79.697%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predSVM, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='SVM (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing 10-Fold Cross Validation on svcClassifier to find the average accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svcaccuracies = cross_val_score(estimator=svcClassifier, X=X_train, y=y_train, cv=10)\n",
    "# svcaccuracies\n",
    "svc10 = cross_val_score(estimator=svcClassifier_unfit, X=X, y=y, cv=10)\n",
    "d.update({'SVC':svc10.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy of Support Vector Machin Classifier is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this works, it takes a relatively long time to converge. SVM is computation intensive. Hence, we need to find a way to reduce the number of input dimensions so that the SVM fitting will converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis:\n",
    "Principal Component Analysis, commonly abbreviated as PCA is a method to reduce the input dimensions without performing feature selection. PCA merely performs a linear transformation of the input data from one form to another. Then generally we only consider the first few resultant principal components as most of the variance of the data will be covered by them. First let us test this on Logistic Regression and then we will apply this to SVM and see if the time taken by the SVM is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca  = PCA(n_components=3, random_state=0)\n",
    "#pca = PCA(.80)\n",
    "# pca.fit(X_train)\n",
    "pca.fit(X)\n",
    "# X_train_PCA = pca.transform(X_train)\n",
    "X_PCA = pca.transform(X)\n",
    "# X_test_PCA = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now only has three columns. This is the result when we take only the first three principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform logistic regression using the principal components we obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaLogRegClassifier = LogisticRegression(random_state=0)\n",
    "pcaLogRegClassifier_unfit = LogisticRegression(random_state=0)\n",
    "\n",
    "tprpcaLR=performCrossValidation(pcaLogRegClassifier,X_PCA,y)\n",
    "# print(tprpcaLR)\n",
    "tpr_val.update({'tprpcaLR':tprpcaLR})\n",
    "tpr_val\n",
    "# print(len(tpr_val))\n",
    "# pcaLogRegClassifier.fit(X_train_PCA, y_train)\n",
    "# y_pred_logReg_PCA = pcaLogRegClassifier.predict(X_test_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the confusion matrix of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaLogRegConfMatrix = confusion_matrix(y_test, y_pred_logReg_PCA)\n",
    "pcaLogRegConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the accuracy is 78.513%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_pred_logReg_PCA, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='PCA Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_logReg_PCA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the average accuracy by using 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcaLogRegaccuracies = cross_val_score(estimator=pcaLogRegClassifier, X=X_train_PCA, y=y_train, cv=10)\n",
    "# pcaLogRegaccuracies\n",
    "pcaLR10= cross_val_score(estimator=pcaLogRegClassifier_unfit, X=X_PCA, y=y, cv=10)\n",
    "d.update({'pcaLR':pcaLR10.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy of Logistic Regression on Principal Component Space is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaLogRegaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us use these same principal components to fit an SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svcPCAclassifier = SVC(kernel='linear', random_state=0)\n",
    "svcPCAclassifier_unfit = SVC(kernel='linear', random_state=0)\n",
    "\n",
    "tprsvcPCA=performCrossValidation(svcPCAclassifier,X_PCA,y)\n",
    "# print(tprsvcPCA)\n",
    "tpr_val.update({'tprsvcPCA':tprsvcPCA})\n",
    "# print(len(tpr_val))\n",
    "# svcPCAclassifier.fit(X_PCA, y_train)\n",
    "# y_pred_svc_PCA = svcPCAclassifier.predict(X_test_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As exected, the SVM took a significantly less time to converge.\n",
    "Let us see the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaSVCConfMatrix = confusion_matrix(y_test, y_pred_svc_PCA)\n",
    "pcaSVCConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the accuracy is 78.797%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_pred_svc_PCA, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='SVM PCA (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svc_PCA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying cross validation on SVC on Principal Component Space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcaSVCaccuracies = cross_val_score(estimator=svcPCAclassifier, X=X_train_PCA, y=y_train, cv=10)\n",
    "# pcaSVCaccuracies\n",
    "pcaSVC10 = cross_val_score(estimator=svcPCAclassifier_unfit, X=X_PCA, y=y, cv=10)\n",
    "d.update({'pcaSVC':pcaSVC10.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average SVC accuracy is given by the mean as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaSVCaccuracies.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Kernel Support Vector Machine with rbf kernel:\n",
    "Kernel Support Vector Machine will introduce the data to a higher dimension and then try to classify them. The essence of the procedure is to make a non-linearly-separable data linearly separable.\n",
    "Here, the kernel function used is set as rbf which is \"Radial Basis Function\". Degree is set as 3 and it is set to use probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "kernelSVCclassifier = SVC(kernel=\"rbf\", degree=3, probability=True, random_state=0)\n",
    "kernelSVCclassifier_unfit = SVC(kernel=\"rbf\", degree=3, probability=True, random_state=0)\n",
    "\n",
    "tprkernelsvc=performCrossValidation(kernelSVCclassifier,X,y)\n",
    "# print(tprkernelsvc)\n",
    "tpr_val.update({'tprkernelsvc':tprkernelsvc})\n",
    "# print(len(tpr_val))\n",
    "tpr_val\n",
    "# kernelSVCclassifier.fit(X_train, y_train)\n",
    "# y_predKernelSVM = kernelSVCclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelSVMconfusionMatrix = confusion_matrix(y_test, y_predKernelSVM)\n",
    "kernelSVMconfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the accuracy is 79.791%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predKernelSVM, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Kernel SVM (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predKernelSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findind the mean accuracy of the model by applying 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernelSVCaccuracies = cross_val_score(estimator=kernelSVCclassifier, X=X_train, y=y_train, cv=10)\n",
    "kernelSVCaccuracies = cross_val_score(estimator=kernelSVCclassifier_unfit, X=X, y=y, cv=10)\n",
    "d.update({'KernelSVA':kernelSVCaccuracies.mean()})\n",
    "d\n",
    "# kernelSVCaccuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy of the kernel SVC is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelSVCaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classification:\n",
    "Decision Tree is the go to method if you want clear insights on how the classifier predicts and under what criteria. This gives clear rules followed by the classifier. Hence, it is not considered as a black box. We are using information gain as the criteria ie criteria = entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tprLogReg': [0.6178343949044586,\n",
       "  0.7114093959731543,\n",
       "  0.65625,\n",
       "  0.6604938271604939,\n",
       "  0.6225165562913907,\n",
       "  0.643312101910828,\n",
       "  0.684931506849315,\n",
       "  0.6975308641975309,\n",
       "  0.65,\n",
       "  0.68],\n",
       " 'tprDecTree': [0.66,\n",
       "  0.73,\n",
       "  0.7010309278350515,\n",
       "  0.7,\n",
       "  0.6808510638297872,\n",
       "  0.6428571428571429,\n",
       "  0.7471264367816092,\n",
       "  0.6076923076923076,\n",
       "  0.723404255319149,\n",
       "  0.76]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decisionTreeClassifierEntropy = DecisionTreeClassifier(criterion=\"entropy\", max_features=10, max_depth=4, random_state=0)\n",
    "decisionTreeClassifierEntropy_unfit = DecisionTreeClassifier(criterion=\"entropy\", max_features=10, random_state=0)\n",
    "\n",
    "tprDecTree=performCrossValidation(decisionTreeClassifierEntropy,X,y)\n",
    "# print(tprDecTree)\n",
    "tpr_val.update({'tprDecTree':tprDecTree})\n",
    "tpr_val\n",
    "# print(len(tpr_val))\n",
    "# decisionTreeClassifierEntropy.fit(X_train, y_train)\n",
    "# y_predDecTreeEntropy = decisionTreeClassifierEntropy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decTreeEntropyConfusionMatrix = confusion_matrix(y_test, y_predDecTreeEntropy)\n",
    "decTreeEntropyConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the accuracy is 72.219%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predDecTreeEntropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dce410b981b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predDecTreeEntropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'darkorange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Decision Tree (area = %0.2f)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'navy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_predDecTreeEntropy' is not defined"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predDecTreeEntropy, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Decision Tree (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predDecTreeEntropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to visualize the generated decision tree but often, it might be too large to be displayed. Also, we need to install pydotplus on our development environment in order for the following snippet to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dotData = StringIO()\n",
    "export_graphviz(decisionTreeClassifierEntropy, out_file=dotData, filled=True, rounded=True, special_characters=True)\n",
    "decTreeEntropyGraph = pydotplus.graph_from_dot_data(dotData.getvalue())\n",
    "Image(decTreeEntropyGraph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, in this case, the graph can be viewed. Also, it is worth noting from the above graph that feature selection is implicit in decision tree, furthermore, the hyperparameter max_features can be used to restrict the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to conclude, we find the average accuracy by 10 Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decTree': 0.7357629143382923}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decTreeAccuracies = cross_val_score(estimator=decisionTreeClassifierEntropy, X=X_train, y=y_train, cv=10)\n",
    "# decTreeAccuracies\n",
    "decTreeAccuracies = cross_val_score(estimator=decisionTreeClassifierEntropy_unfit, X=X, y=y, cv=10)\n",
    "d.update({'decTree':decTreeAccuracies.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy of the decision tree classifier is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decTreeAccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classification:\n",
    "\n",
    "Random forest comes under a category called ensemble algorithms. Here, instead of one single decision tree, an army of decision trees are used and the results learned by them are then converged together to create the model. The hyperparameter n_estimators can be used to set the number of individual decision trees used in the forest. We are using information gain as criteria as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tprLogReg': [0.6178343949044586,\n",
       "  0.7114093959731543,\n",
       "  0.65625,\n",
       "  0.6604938271604939,\n",
       "  0.6225165562913907,\n",
       "  0.643312101910828,\n",
       "  0.684931506849315,\n",
       "  0.6975308641975309,\n",
       "  0.65,\n",
       "  0.68],\n",
       " 'tprDecTree': [0.66,\n",
       "  0.73,\n",
       "  0.7010309278350515,\n",
       "  0.7,\n",
       "  0.6808510638297872,\n",
       "  0.6428571428571429,\n",
       "  0.7471264367816092,\n",
       "  0.6076923076923076,\n",
       "  0.723404255319149,\n",
       "  0.76],\n",
       " 'tprRandomForest': [0.6148648648648649,\n",
       "  0.7153846153846154,\n",
       "  0.6171875,\n",
       "  0.6048387096774194,\n",
       "  0.5555555555555556,\n",
       "  0.5759493670886076,\n",
       "  0.6201550387596899,\n",
       "  0.6482758620689655,\n",
       "  0.6541353383458647,\n",
       "  0.6259541984732825]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=10, criterion=\"entropy\", max_features=15, random_state=0)\n",
    "randomForestClassifier_unfit = RandomForestClassifier(n_estimators=10, criterion=\"entropy\", max_features=15, random_state=0)\n",
    "\n",
    "tprRandomForest=performCrossValidation(randomForestClassifier,X,y)\n",
    "# print(tprRandomForest)\n",
    "tpr_val.update({'tprRandomForest':tprRandomForest})\n",
    "# print(len(tpr_val))\n",
    "tpr_val\n",
    "# randomForestClassifier.fit(X_train, y_train)\n",
    "# y_predRandomForest = randomForestClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for the model is computed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestConfMatrix = confusion_matrix(y_test, y_predRandomForest)\n",
    "randomForestConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, accuracy is 78.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predRandomForest, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Random Forest (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, random forest is giving a much higher auc score compared to decision tree. Ten trees are better than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predRandomForest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the mean accuracy of random forest classifier by 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomForestaccuracies = cross_val_score(estimator=randomForestClassifier, X=X_train, y=y_train, cv=10)\n",
    "# randomForestaccuracies\n",
    "randomForestaccuracies = cross_val_score(estimator=randomForestClassifier_unfit, X=X, y=y, cv=10)\n",
    "d.update({'RandForest':randomForestaccuracies.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean accuracy is givevn by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Method:\n",
    "Light GBM is a gradient boosting framework that uses tree based learning algorithm. Light GBM grows tree vertically (leaf wise) while other algorithm grows trees horizontally (level wise). It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from lightgbm import LGBMClassifier\n",
    "# lgbmClassifier = LGBMClassifier(learning_rate=0.1, objective=\"binary\", random_state=0, max_depth=12)\n",
    "# lgbmClassifier.fit(X_train, y_train)\n",
    "# y_predLGBM = lgbmClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for lgbm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbmConfMatrix = confusion_matrix(y_test, y_predLGBM)\n",
    "# lgbmConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the accuracy is 79.981%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc and finding the auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = roc_curve(y_predLGBM, y_test)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure(figsize = (10,6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=1, label='Light Gradient Boosting (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_predLGBM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the mean accuracy by 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbmaccuracies = cross_val_score(estimator=lgbmClassifier, X=X_train, y=y_train, cv=10)\n",
    "# lgbmaccuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean average achieved from lgbm is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbmaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xtreme Gradient Boosting:\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbClassifier = XGBClassifier(learning_rate=0.1, max_depth=4)\n",
    "# xgbClassifier_unfit = XGBClassifier(learning_rate=0.1, max_depth=4)\n",
    "\n",
    "xgbClassifier_unfit = XGBClassifier(learning_rate=0.1, max_depth=4)\n",
    "\n",
    "tprxbg=performCrossValidation(xgbClassifier,X,y)\n",
    "# prin/t(tprxbg)\n",
    "tpr_val.update({'tprxbg':tprxbg})\n",
    "tpr_val\n",
    "# print(len(tpr_val))\n",
    "# xgbClassifier.fit(X_train, y_train)\n",
    "# y_predXGB = xgbClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbConfMatrix = confusion_matrix(y_test, y_predXGB)\n",
    "xgbConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, accuracy is 80.785%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predXGB, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='XG Boost (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predXGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying 10 fold cross validation on the xgbClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbAccuracies = cross_val_score(estimator=xgbClassifier, X=X_train, y=y_train, cv=10)\n",
    "# xgbAccuracies\n",
    "xgbAccuracies = cross_val_score(estimator=xgbClassifier_unfit, X=X, y=y, cv=10)\n",
    "d.update({'XGB':xgbAccuracies.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy achieved by the xgboost classifier is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbAccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron:\n",
    "A multilayer perceptron is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. MLP can work with non-linearly-separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# nnClassifier = MLPClassifier(activation=\"relu\", solver=\"sgd\", max_iter=85, random_state=0, verbose=False) \n",
    "nnClassifier_unfit = MLPClassifier(activation=\"relu\", solver=\"sgd\", max_iter=85, random_state=0, verbose=False) \n",
    "\n",
    "tprNN=performCrossValidation(nnClassifier,X,y)\n",
    "# print(tppdarNN)\n",
    "tpr_val.update({'tprNN':tprNN})\n",
    "tpr_val\n",
    "# print(len(tpr_val))\n",
    "# nnClassifier.fit(X_train, y_train)\n",
    "# y_predMLP = nnClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpConfMatrix = confusion_matrix(y_test, y_predMLP)\n",
    "mlpConfMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, accuracy is 79.933%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predMLP, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='MLP Classifier (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predMLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the average accuracy of MLP classifier by applying 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlpaccuracies = cross_val_score(estimator=nnClassifier, X=X_train, y=y_train, cv=10)\n",
    "# mlpaccuracies\n",
    "mlpaccuracies = cross_val_score(estimator=nnClassifier_unfit, X=X, y=y, cv=10)\n",
    "d.update({'MLP':mlpaccuracies.mean()})\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy of MLP classifier is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d)\n",
    "\n",
    "print(\"tpr values:\")\n",
    "print(tpr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpaccuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with all the models. Let us compare the models we generated by plotting the models' roc on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_predLogReg, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--') # Straight\n",
    "fpr, tpr, thresholds = roc_curve(y_predNB, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='blue', lw=1, label='Naive Bayes (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predKNN, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='green', lw=1, label='KNN (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_pred_logReg_PCA, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='cyan', lw=1, label='LogReg PCA (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predSVM, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='teal', lw=1, label='SVM (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_pred_svc_PCA, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='magenta', lw=1, label='SVC PCA (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predKernelSVM, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='red', lw=1, label='Kernel SVM (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predDecTreeEntropy, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='yellow', lw=1, label='Decision Tree (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predRandomForest, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='black', lw=1, label='Random Forest (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predLGBM, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='violet', lw=1, label='Light Gradient Boosting (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predXGB, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='darkgoldenrod', lw=1, label='XG Boosting (area = %0.2f)' % roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_predMLP, y_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='chocolate', lw=1, label='MLP (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as expected, xgboost has the highest score. It is closely followed by MLP, LGBM, KernelSVM, SVM and of course Logistic Regression. The worst score is for Decision Tree and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection:\n",
    "\n",
    "We now have a total of twelve models of varying accuracies. the question now is - \"Which model do we select for final deployment so that we can use it to predict the outcome of unseen data. We are using the t statistical test for evaluating the models pairwise with a confidencec level of 5% which is a threshold of 2.82. If the value we get after doing the t test for two models is greater than 2.82, it means that the two models are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findT(accuracy1, accuracy2):\n",
    "    differences = []\n",
    "    for i in range(10):\n",
    "        differences.append(abs(accuracy1[i]-accuracy2[i]))\n",
    "    meanDiff = mean(differences)\n",
    "    varDiff = pvariance(differences)\n",
    "    t = meanDiff/sqrt(varDiff/10)\n",
    "    if t < 2.82: # 5% confidence level\n",
    "        return False, t\n",
    "    return True, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing logistic regression model with and without principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On comparison, we are seeing that for the confidence level of 5%, the two models differ. Since, the accuracy of logistic regression in the input space is higher than that of principal component space, we will choose the default logistic regression over the on PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing logistic regression model with naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge difference between the predictive power of logistic regression and naive bayes models. We are choosing logistic regression as it has higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing logistic regression with k nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing a much lower difference but still a difference nontheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing logistic regression with kernel support vector machine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tpr_val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprnb'],tpr_val['tprLogReg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprLogReg'],tpr_val['tprpcaLR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprSVC'],tpr_val['tprKNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprDecTree'],tpr_val['tprRandomForest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprLogReg'],tpr_val['tprNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprxbg'],tpr_val['tprLogReg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findT(tpr_val['tprxbg'],tpr_val['tprNN'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
